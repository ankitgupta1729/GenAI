{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5cdc4ec-bb7c-488b-b00b-d6201faecb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (23.3.2)\n",
      "Collecting pip\n",
      "  Downloading pip-24.2-py3-none-any.whl.metadata (3.6 kB)\n",
      "Downloading pip-24.2-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.3.2\n",
      "    Uninstalling pip-23.3.2:\n",
      "      Successfully uninstalled pip-23.3.2\n",
      "Successfully installed pip-24.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b5e413e-40cf-4119-8a2e-ef02aae275b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogluon-multimodal 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires pytorch-lightning<1.10.0,>=1.9.0, but you have pytorch-lightning 2.0.9 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torchmetrics<0.12.0,>=0.11.0, but you have torchmetrics 1.0.3 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torchvision<0.15.0, but you have torchvision 0.15.2a0+ab7b3e6 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires pytorch-lightning<1.10.0,>=1.7.4, but you have pytorch-lightning 2.0.9 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install --disable-pip-version-check torch==1.13.1 torchdata==0.5.1 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37ae971c-5f3d-4c58-962c-5922156cd74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogluon-multimodal 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires pytorch-lightning<1.10.0,>=1.9.0, but you have pytorch-lightning 2.0.9 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torchmetrics<0.12.0,>=0.11.0, but you have torchmetrics 1.0.3 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torchvision<0.15.0, but you have torchvision 0.15.2a0+ab7b3e6 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires transformers[sentencepiece]<4.41.0,>=4.36.0, but you have transformers 4.27.2 which is incompatible.\n",
      "pathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.6 which is incompatible.\n",
      "pathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.14 which is incompatible.\n",
      "sagemaker 2.227.0 requires boto3<2.0,>=1.34.142, but you have boto3 1.34.131 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install transformers==4.27.2 datasets==2.11.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "face1c79-ce0e-4199-8f30-0ff99234a43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543c2688-7794-42d2-bdd0-fe472160d4ee",
   "metadata": {},
   "source": [
    "## Summarize Dialogue without Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8167c12-6699-4654-8f96-62ecf144fa94",
   "metadata": {},
   "source": [
    "### Here we will be generating a summary of a dialogue with the pre-trained LLM FLAN-T5 from Hugging Face\n",
    "### Let's upload some simple dialogues from DialogSum Hugging Face dataset. This dataset contains 10k+ dialogues with the corresponding manually labelled summaries and topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3542af3a-e0bd-4b33-bb30-f8bc431c7e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/sagemaker-user/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd36827d3490488d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c43355159da4114bf93bac471e1fa57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "huggingface_dataset_name='knkarthick/dialogsum'\n",
    "dataset= load_dataset(huggingface_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "714b5185-d910-43d9-8ba2-77dcc42f6183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT DIALOGUE:\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT DIALOGUE:\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## print a couple of dialogues with their baseline summaries\n",
    "example_indices =[40,200]\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "for i,index in enumerate(example_indices):\n",
    "    print(dash_line)\n",
    "    print('Example ',i+1)\n",
    "    print(dash_line)\n",
    "    print('INPUT DIALOGUE:')\n",
    "    print(dataset['test'][index]['dialogue'])\n",
    "    print(dash_line)\n",
    "    print('BASELINE HUMAN SUMMARY:')\n",
    "    print(dataset['test'][index]['summary'])\n",
    "    print(dash_line)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8eb8dc10-7209-49f2-ad2b-3f5124d5be14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will improve the summary by our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d0b847-792b-44fa-9e1b-7e7e4b96af33",
   "metadata": {},
   "source": [
    "### Load the Flan-T5 model, create an instance of the AutoModelForSeq2SeqLM class with .from_pretrained() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18577e1a-e492-4f91-bd91-7ed7aa03ce5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name='google/flan-t5-base'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120938d6-0c8d-4573-9da7-794c311a9535",
   "metadata": {},
   "source": [
    "#### To perform encoding and decoding, you need the text in tokenized form. Tokenization is the process of spliting text into smaller units that can be processed by LLM models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b93026e-314f-4fec-9e2b-d44fd68500cf",
   "metadata": {},
   "source": [
    "#### Download the tokenizer for flan-t5 model using AutoTokenizer.from_pretrained() method. Parameter use_fast switches on fast tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7efb09f5-f1fe-454c-beb3-24264adfdfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name,use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "496b1cdc-9cb4-4a95-bc64-efa783c3a43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## These are all from the hugging face transformer library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12481c7b-ffb3-4456-99ca-1fef7121fd96",
   "metadata": {},
   "source": [
    "#### Test the tokenizer encoding and decoding a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0be7df09-f9e8-4094-9f0a-33d498d117e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 03:44:29.778213: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Sentence:\n",
      "tensor([ 363,   97,   19,   34,    6, 3059,   58,    1])\n",
      "\n",
      " Decoded Sentence:\n",
      "What time is it, Tom?\n"
     ]
    }
   ],
   "source": [
    "sentence = 'What time is it, Tom?'\n",
    "sentence_encoded = tokenizer(sentence,return_tensors='pt')\n",
    "\n",
    "sentence_decoded = tokenizer.decode(sentence_encoded['input_ids'][0],skip_special_tokens=True)\n",
    "print(\"Encoded Sentence:\")\n",
    "print(sentence_encoded['input_ids'][0])\n",
    "print('\\n Decoded Sentence:')\n",
    "print(sentence_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbe4dbc-90ed-4613-8974-f27aa0902c9c",
   "metadata": {},
   "source": [
    "#### Now, it's time to explore how well the base LLM summarizes a dialogue without any prompt engineering. Prompt engineering is an act of human changing the prompt(input) to improve the response for a given task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50678815-1810-4c3f-9408-df53ba7a4da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n",
      " Person1: It's ten to nine.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n",
      " #Person1#: I'm thinking of upgrading my computer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "    inputs = tokenizer(dialogue,return_tensors='pt')\n",
    "    output = tokenizer.decode(model.generate(inputs['input_ids'],max_new_tokens=50,)[0],skip_special_tokens=True)\n",
    "    print(dash_line)\n",
    "    print('Example ',i+1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{dialogue}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\\n {output}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d53e328-84f2-41c5-8960-54c426956484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is not doing a very good job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c082903f-4729-4363-b341-2c845cd67137",
   "metadata": {},
   "source": [
    "## Summarize Dialogue with an Instruction Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07d7ebaa-34e1-4d6b-b4f9-4d3959d04b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prompt engineering is an important concept in using foundation models for text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262acf06-63a1-4b26-bd99-623a3e1f800e",
   "metadata": {},
   "source": [
    "### Zero Shot Inference with an Instruction Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b31decd-fb12-4567-a9aa-4392b17a6973",
   "metadata": {},
   "source": [
    "#### In order to instruct the model to perform a task - summarise a dialogue - you can take the dialogue and convert it into an instruction prompt. This is often called zero shot inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e5da59c-0dad-44e5-b996-5c4745eb4467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "\n",
      "    Summarize the following conversation.\n",
      "    #Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "    Summary:\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      " The train is about to leave.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "\n",
      "    Summarize the following conversation.\n",
      "    #Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "    Summary:\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      " #Person1#: I'm thinking of upgrading my computer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "\n",
    "    prompt= f\"\"\"\n",
    "\n",
    "    Summarize the following conversation.\n",
    "    {dialogue}\n",
    "    Summary:\n",
    "    \"\"\"\n",
    "    #input constructed prompt instead of the dialogue.\n",
    "    inputs = tokenizer(prompt,return_tensors='pt')\n",
    "    output = tokenizer.decode(model.generate(inputs['input_ids'],max_new_tokens=50,)[0],skip_special_tokens=True)\n",
    "    print(dash_line)\n",
    "    print('Example ',i+1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{prompt}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENERATION - ZERO SHOT:\\n {output}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68047ca4-2a5b-48bb-8d88-37a5c407cfca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "\n",
      "    Dialogue:\n",
      "    #Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "\n",
      "    What was going on?\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      " Tom is late for the train.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "\n",
      "    Dialogue:\n",
      "    #Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "    What was going on?\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      " #Person1#: You could add a painting program to your software. #Person2#: That would be a bonus. #Person1#: You might also want to upgrade your hardware. #Person1#\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "\n",
    "    prompt= f\"\"\"\n",
    "\n",
    "    Dialogue:\n",
    "    {dialogue}\n",
    "\n",
    "    What was going on?\n",
    "    \"\"\"\n",
    "    #input constructed prompt instead of the dialogue.\n",
    "    inputs = tokenizer(prompt,return_tensors='pt')\n",
    "    output = tokenizer.decode(model.generate(inputs['input_ids'],max_new_tokens=50,)[0],skip_special_tokens=True)\n",
    "    print(dash_line)\n",
    "    print('Example ',i+1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{prompt}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENERATION - ZERO SHOT:\\n {output}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbaea4b-9048-4ce0-82e4-277b1f93e3d8",
   "metadata": {},
   "source": [
    "## Summarize dialogue with One Shot and Few Shot Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35aef068-1733-45c0-ac90-a226d0f712ec",
   "metadata": {},
   "source": [
    "#### One shot and few shot inference are the practices of providing an LLM with either one or more full examples of prompt-response pairs that match your task before your actual prompt that you want completed. This is called \"in-context\" learning and puts your model into a state that understands your specific task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae49b6ab-82ab-4339-b9eb-0f4e05e1e944",
   "metadata": {},
   "outputs": [],
   "source": [
    "## One Shot Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8318ed4-2053-408b-801d-3d9c309cf948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(example_indices_full,example_index_to_summarize):\n",
    "    prompt = ''\n",
    "    for index in example_indices_full:\n",
    "        dialogue = dataset['test'][index]['dialogue']\n",
    "        summary = dataset['test'][index]['summary']\n",
    "\n",
    "        # The stop sequence '{summary}\\n\\n\\n' is important for FLAN-T5. Other models may have their own preferred stop sequence\n",
    "        prompt += f\"\"\"\n",
    "        Dialogue:\n",
    "        {dialogue}\n",
    "\n",
    "        What was going on?\n",
    "        {summary}\n",
    "        \"\"\"\n",
    "\n",
    "    dialogue = dataset['test'][example_index_to_summarize]['dialogue']\n",
    "    prompt += f\"\"\"\n",
    "\n",
    "    Dialogue:\n",
    "    {dialogue}\n",
    "    What was going on?\n",
    "    \"\"\"\n",
    "   \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0eb8316-0fe5-4037-b9b2-08fdcf3721ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the prompt to perform one shot inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "315613be-2ce6-473d-a84a-d0fea12b05e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Dialogue:\n",
      "        #Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "\n",
      "        What was going on?\n",
      "        #Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "        \n",
      "\n",
      "    Dialogue:\n",
      "    #Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "    What was going on?\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "example_indices_full =[40]\n",
    "example_index_to_summarize = 200\n",
    "one_shot_prompt = make_prompt(example_indices_full,example_index_to_summarize)\n",
    "print(one_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72160556-d3e8-465e-ac07-ce00e26bc2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now pass the prompt to perform the one shot inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db4be0c7-dd0f-4908-a5ba-7c65a42f3d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ONE SHOT:\n",
      " #Person1 wants to upgrade his system. #Person2 wants to add a painting program to his software. #Person1 wants to add a CD-ROM drive.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary = dataset['test'][example_index_to_summarize]['summary']\n",
    "\n",
    "inputs = tokenizer(one_shot_prompt, return_tensors=\"pt\")\n",
    "output = tokenizer.decode(model.generate(inputs['input_ids'],max_new_tokens=50,)[0],skip_special_tokens=True)\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ONE SHOT:\\n {output}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e99c750-8881-49b6-bfdf-7e8795406344",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prompt engineering, zero shot, one shot, few shot inferences are the first step always if we have a hub of models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04900627-56ea-435f-b0a2-f7cdb08908f5",
   "metadata": {},
   "source": [
    "## Few Shot Inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71ce00e7-a7f7-40dd-b5a9-fcab66ca2533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Dialogue:\n",
      "        #Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "\n",
      "        What was going on?\n",
      "        #Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "        \n",
      "        Dialogue:\n",
      "        #Person1#: May, do you mind helping me prepare for the picnic?\n",
      "#Person2#: Sure. Have you checked the weather report?\n",
      "#Person1#: Yes. It says it will be sunny all day. No sign of rain at all. This is your father's favorite sausage. Sandwiches for you and Daniel.\n",
      "#Person2#: No, thanks Mom. I'd like some toast and chicken wings.\n",
      "#Person1#: Okay. Please take some fruit salad and crackers for me.\n",
      "#Person2#: Done. Oh, don't forget to take napkins disposable plates, cups and picnic blanket.\n",
      "#Person1#: All set. May, can you help me take all these things to the living room?\n",
      "#Person2#: Yes, madam.\n",
      "#Person1#: Ask Daniel to give you a hand?\n",
      "#Person2#: No, mom, I can manage it by myself. His help just causes more trouble.\n",
      "\n",
      "        What was going on?\n",
      "        Mom asks May to help to prepare for the picnic and May agrees.\n",
      "        \n",
      "        Dialogue:\n",
      "        #Person1#: Hello, I bought the pendant in your shop, just before. \n",
      "#Person2#: Yes. Thank you very much. \n",
      "#Person1#: Now I come back to the hotel and try to show it to my friend, the pendant is broken, I'm afraid. \n",
      "#Person2#: Oh, is it? \n",
      "#Person1#: Would you change it to a new one? \n",
      "#Person2#: Yes, certainly. You have the receipt? \n",
      "#Person1#: Yes, I do. \n",
      "#Person2#: Then would you kindly come to our shop with the receipt by 10 o'clock? We will replace it. \n",
      "#Person1#: Thank you so much. \n",
      "\n",
      "        What was going on?\n",
      "        #Person1# wants to change the broken pendant in #Person2#'s shop.\n",
      "        \n",
      "\n",
      "    Dialogue:\n",
      "    #Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "    What was going on?\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# we are helping the model\n",
    "example_indices_full = [40,80,120]\n",
    "example_index_to_summarize = 200\n",
    "few_shot_prompt = make_prompt(example_indices_full,example_index_to_summarize)\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e203e0ac-d08e-4207-a289-261a1c2a3212",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(max_new_tokens=50,do_sample=True,temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c6f73e2-7708-4069-9201-2595d2196b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - FEW SHOT:\n",
      " #Person1 suggests the following software enhancements to their software: adding a painting program, adding a computer software software, speeding up the computer, adding a CD, adding a CD-ROM drive.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary = dataset['test'][example_index_to_summarize]['summary']\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors=\"pt\")\n",
    "output = tokenizer.decode(model.generate(inputs['input_ids'],generation_config,)[0],skip_special_tokens=True)\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n {output}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0ce8a7-0285-415a-bd88-ab5b87382105",
   "metadata": {},
   "source": [
    "#### In this case few shot did not provide much of an improvement over one shot inference. And, anything above 5 or 6 shot will typically not help much, either. Also, you need to ensure that you do not exceed the model's input-context length which, in our case, if 512 tokens. Anything above the context length will be ignored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662ebdeb-b4e3-4dc4-9bae-92932324d3a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea13d5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (23.3.2)\n",
      "Collecting pip\n",
      "  Downloading pip-24.2-py3-none-any.whl.metadata (3.6 kB)\n",
      "Downloading pip-24.2-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.3.2\n",
      "    Uninstalling pip-23.3.2:\n",
      "      Successfully uninstalled pip-23.3.2\n",
      "Successfully installed pip-24.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8cb99f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogluon-multimodal 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires pytorch-lightning<1.10.0,>=1.9.0, but you have pytorch-lightning 2.0.9 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torchmetrics<0.12.0,>=0.11.0, but you have torchmetrics 1.0.3 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torchvision<0.15.0, but you have torchvision 0.15.2a0+ab7b3e6 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires pytorch-lightning<1.10.0,>=1.7.4, but you have pytorch-lightning 2.0.9 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install --disable-pip-version-check torch==1.13.1 torchdata==0.5.1 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63628488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogluon-multimodal 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires pytorch-lightning<1.10.0,>=1.9.0, but you have pytorch-lightning 2.0.9 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torchmetrics<0.12.0,>=0.11.0, but you have torchmetrics 1.0.3 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torchvision<0.15.0, but you have torchvision 0.15.2a0+ab7b3e6 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires transformers[sentencepiece]<4.41.0,>=4.36.0, but you have transformers 4.27.2 which is incompatible.\n",
      "pathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.6 which is incompatible.\n",
      "pathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.14 which is incompatible.\n",
      "sagemaker 2.227.0 requires boto3<2.0,>=1.34.142, but you have boto3 1.34.131 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install transformers==4.27.2 datasets==2.11.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93d4f292",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe0c8b9",
   "metadata": {},
   "source": [
    "## Summarize Dialogue without Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154a8887",
   "metadata": {},
   "source": [
    "### Here we will be generating a summary of a dialogue with the pre-trained LLM FLAN-T5 from Hugging Face\n",
    "### Let's upload some simple dialogues from DialogSum Hugging Face dataset. This dataset contains 10k+ dialogues with the corresponding manually labelled summaries and topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a29b9f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/sagemaker-user/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd36827d3490488d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c43355159da4114bf93bac471e1fa57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "huggingface_dataset_name='knkarthick/dialogsum'\n",
    "dataset= load_dataset(huggingface_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f7da4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT DIALOGUE:\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT DIALOGUE:\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## print a couple of dialogues with their baseline summaries\n",
    "example_indices =[40,200]\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "for i,index in enumerate(example_indices):\n",
    "    print(dash_line)\n",
    "    print('Example ',i+1)\n",
    "    print(dash_line)\n",
    "    print('INPUT DIALOGUE:')\n",
    "    print(dataset['test'][index]['dialogue'])\n",
    "    print(dash_line)\n",
    "    print('BASELINE HUMAN SUMMARY:')\n",
    "    print(dataset['test'][index]['summary'])\n",
    "    print(dash_line)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2d8cdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will improve the summary by our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bd25bf",
   "metadata": {},
   "source": [
    "### Load the Flan-T5 model, create an instance of the AutoModelForSeq2SeqLM class with .from_pretrained() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1790e560",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name='google/flan-t5-base'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f055036",
   "metadata": {},
   "source": [
    "#### To perform encoding and decoding, you need the text in tokenized form. Tokenization is the process of spliting text into smaller units that can be processed by LLM models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc37f46e",
   "metadata": {},
   "source": [
    "#### Download the tokenizer for flan-t5 model using AutoTokenizer.from_pretrained() method. Parameter use_fast switches on fast tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21f501a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name,use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f86e96bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## These are all from the hugging face transformer library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41131024",
   "metadata": {},
   "source": [
    "#### Test the tokenizer encoding and decoding a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13d802f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 03:44:29.778213: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Sentence:\n",
      "tensor([ 363,   97,   19,   34,    6, 3059,   58,    1])\n",
      "\n",
      " Decoded Sentence:\n",
      "What time is it, Tom?\n"
     ]
    }
   ],
   "source": [
    "sentence = 'What time is it, Tom?'\n",
    "sentence_encoded = tokenizer(sentence,return_tensors='pt')\n",
    "\n",
    "sentence_decoded = tokenizer.decode(sentence_encoded['input_ids'][0],skip_special_tokens=True)\n",
    "print(\"Encoded Sentence:\")\n",
    "print(sentence_encoded['input_ids'][0])\n",
    "print('\\n Decoded Sentence:')\n",
    "print(sentence_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123ce14c",
   "metadata": {},
   "source": [
    "#### Now, it's time to explore how well the base LLM summarizes a dialogue without any prompt engineering. Prompt engineering is an act of human changing the prompt(input) to improve the response for a given task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca3049e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n",
      " Person1: It's ten to nine.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n",
      " #Person1#: I'm thinking of upgrading my computer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "    inputs = tokenizer(dialogue,return_tensors='pt')\n",
    "    output = tokenizer.decode(model.generate(inputs['input_ids'],max_new_tokens=50,)[0],skip_special_tokens=True)\n",
    "    print(dash_line)\n",
    "    print('Example ',i+1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{dialogue}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\\n {output}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7b9308a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is not doing a very good job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05baf25b",
   "metadata": {},
   "source": [
    "## Summarize Dialogue with an Instruction Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c24de0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prompt engineering is an important concept in using foundation models for text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff3aaf2",
   "metadata": {},
   "source": [
    "### Zero Shot Inference with an Instruction Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cb71ca",
   "metadata": {},
   "source": [
    "#### In order to instruct the model to perform a task - summarise a dialogue - you can take the dialogue and convert it into an instruction prompt. This is often called zero shot inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd326f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "\n",
      "    Summarize the following conversation.\n",
      "    #Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "    Summary:\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      " The train is about to leave.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "\n",
      "    Summarize the following conversation.\n",
      "    #Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "    Summary:\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      " #Person1#: I'm thinking of upgrading my computer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "\n",
    "    prompt= f\"\"\"\n",
    "\n",
    "    Summarize the following conversation.\n",
    "    {dialogue}\n",
    "    Summary:\n",
    "    \"\"\"\n",
    "    #input constructed prompt instead of the dialogue.\n",
    "    inputs = tokenizer(prompt,return_tensors='pt')\n",
    "    output = tokenizer.decode(model.generate(inputs['input_ids'],max_new_tokens=50,)[0],skip_special_tokens=True)\n",
    "    print(dash_line)\n",
    "    print('Example ',i+1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{prompt}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENERATION - ZERO SHOT:\\n {output}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5146f516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "\n",
      "    Dialogue:\n",
      "    #Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "\n",
      "    What was going on?\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      " Tom is late for the train.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "\n",
      "    Dialogue:\n",
      "    #Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "    What was going on?\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      " #Person1#: You could add a painting program to your software. #Person2#: That would be a bonus. #Person1#: You might also want to upgrade your hardware. #Person1#\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "\n",
    "    prompt= f\"\"\"\n",
    "\n",
    "    Dialogue:\n",
    "    {dialogue}\n",
    "\n",
    "    What was going on?\n",
    "    \"\"\"\n",
    "    #input constructed prompt instead of the dialogue.\n",
    "    inputs = tokenizer(prompt,return_tensors='pt')\n",
    "    output = tokenizer.decode(model.generate(inputs['input_ids'],max_new_tokens=50,)[0],skip_special_tokens=True)\n",
    "    print(dash_line)\n",
    "    print('Example ',i+1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{prompt}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENERATION - ZERO SHOT:\\n {output}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37d7ca1",
   "metadata": {},
   "source": [
    "## Summarize dialogue with One Shot and Few Shot Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060213b3",
   "metadata": {},
   "source": [
    "#### One shot and few shot inference are the practices of providing an LLM with either one or more full examples of prompt-response pairs that match your task before your actual prompt that you want completed. This is called \"in-context\" learning and puts your model into a state that understands your specific task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33c666f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## One Shot Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3bbcd901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(example_indices_full,example_index_to_summarize):\n",
    "    prompt = ''\n",
    "    for index in example_indices_full:\n",
    "        dialogue = dataset['test'][index]['dialogue']\n",
    "        summary = dataset['test'][index]['summary']\n",
    "\n",
    "        # The stop sequence '{summary}\\n\\n\\n' is important for FLAN-T5. Other models may have their own preferred stop sequence\n",
    "        prompt += f\"\"\"\n",
    "        Dialogue:\n",
    "        {dialogue}\n",
    "\n",
    "        What was going on?\n",
    "        {summary}\n",
    "        \"\"\"\n",
    "\n",
    "    dialogue = dataset['test'][example_index_to_summarize]['dialogue']\n",
    "    prompt += f\"\"\"\n",
    "\n",
    "    Dialogue:\n",
    "    {dialogue}\n",
    "    What was going on?\n",
    "    \"\"\"\n",
    "   \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b735d696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the prompt to perform one shot inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53e3042d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Dialogue:\n",
      "        #Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "\n",
      "        What was going on?\n",
      "        #Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "        \n",
      "\n",
      "    Dialogue:\n",
      "    #Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "    What was going on?\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "example_indices_full =[40]\n",
    "example_index_to_summarize = 200\n",
    "one_shot_prompt = make_prompt(example_indices_full,example_index_to_summarize)\n",
    "print(one_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77ec56de",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now pass the prompt to perform the one shot inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6291003f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ONE SHOT:\n",
      " #Person1 wants to upgrade his system. #Person2 wants to add a painting program to his software. #Person1 wants to add a CD-ROM drive.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary = dataset['test'][example_index_to_summarize]['summary']\n",
    "\n",
    "inputs = tokenizer(one_shot_prompt, return_tensors=\"pt\")\n",
    "output = tokenizer.decode(model.generate(inputs['input_ids'],max_new_tokens=50,)[0],skip_special_tokens=True)\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ONE SHOT:\\n {output}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10326de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prompt engineering, zero shot, one shot, few shot inferences are the first step always if we have a hub of models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be90ea7",
   "metadata": {},
   "source": [
    "## Few Shot Inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ee1e473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Dialogue:\n",
      "        #Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "\n",
      "        What was going on?\n",
      "        #Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "        \n",
      "        Dialogue:\n",
      "        #Person1#: May, do you mind helping me prepare for the picnic?\n",
      "#Person2#: Sure. Have you checked the weather report?\n",
      "#Person1#: Yes. It says it will be sunny all day. No sign of rain at all. This is your father's favorite sausage. Sandwiches for you and Daniel.\n",
      "#Person2#: No, thanks Mom. I'd like some toast and chicken wings.\n",
      "#Person1#: Okay. Please take some fruit salad and crackers for me.\n",
      "#Person2#: Done. Oh, don't forget to take napkins disposable plates, cups and picnic blanket.\n",
      "#Person1#: All set. May, can you help me take all these things to the living room?\n",
      "#Person2#: Yes, madam.\n",
      "#Person1#: Ask Daniel to give you a hand?\n",
      "#Person2#: No, mom, I can manage it by myself. His help just causes more trouble.\n",
      "\n",
      "        What was going on?\n",
      "        Mom asks May to help to prepare for the picnic and May agrees.\n",
      "        \n",
      "        Dialogue:\n",
      "        #Person1#: Hello, I bought the pendant in your shop, just before. \n",
      "#Person2#: Yes. Thank you very much. \n",
      "#Person1#: Now I come back to the hotel and try to show it to my friend, the pendant is broken, I'm afraid. \n",
      "#Person2#: Oh, is it? \n",
      "#Person1#: Would you change it to a new one? \n",
      "#Person2#: Yes, certainly. You have the receipt? \n",
      "#Person1#: Yes, I do. \n",
      "#Person2#: Then would you kindly come to our shop with the receipt by 10 o'clock? We will replace it. \n",
      "#Person1#: Thank you so much. \n",
      "\n",
      "        What was going on?\n",
      "        #Person1# wants to change the broken pendant in #Person2#'s shop.\n",
      "        \n",
      "\n",
      "    Dialogue:\n",
      "    #Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "    What was going on?\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# we are helping the model\n",
    "example_indices_full = [40,80,120]\n",
    "example_index_to_summarize = 200\n",
    "few_shot_prompt = make_prompt(example_indices_full,example_index_to_summarize)\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ff983b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(max_new_tokens=50,do_sample=True,temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad963444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - FEW SHOT:\n",
      " #Person1 suggests the following software enhancements to their software: adding a painting program, adding a computer software software, speeding up the computer, adding a CD, adding a CD-ROM drive.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary = dataset['test'][example_index_to_summarize]['summary']\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors=\"pt\")\n",
    "output = tokenizer.decode(model.generate(inputs['input_ids'],generation_config,)[0],skip_special_tokens=True)\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n {output}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27330ac2",
   "metadata": {},
   "source": [
    "#### In this case few shot did not provide much of an improvement over one shot inference. And, anything above 5 or 6 shot will typically not help much, either. Also, you need to ensure that you do not exceed the model's input-context length which, in our case, if 512 tokens. Anything above the context length will be ignored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad9d2cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
